{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c902ffd-ef46-4aa4-81de-1a46a21a4210",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c23687e-dc70-47b8-87a9-d07efcc30955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Dropout, Dense\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, ttk\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fe04d5-76b2-46cc-a28d-4cd58491696c",
   "metadata": {},
   "source": [
    "## Set Paths and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ec20d37-d423-46fd-a814-b0dfd4f6cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = r'G:\\Capstone data\\(Enhanced)_models\\ViT_fold4.h5'\n",
    "ALL_SENTENCES_PATH = r'G:\\Capstone data\\all_Sentences.xlsx'\n",
    "LABELS_PATH = r'G:\\Capstone data\\KARSL-502_Labels.xlsx'\n",
    "BACKGROUND_PATH = r\"G:\\Capstone data\\background.png\"\n",
    "\n",
    "F_AVG = 30  # Number of frames (time steps) expected by model\n",
    "MIN_IMAGES = 2  # Minimum number of images\n",
    "MAX_IMAGES = 5  # Maximum number of images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f5673-8b35-4fac-82f3-7151881bd2fc",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e439c42-196d-408a-8bd3-0a900dff4536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation='relu'),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x_norm = self.layernorm1(inputs)\n",
    "        attn_output = self.att(x_norm, x_norm)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = inputs + attn_output\n",
    "        \n",
    "        out_norm = self.layernorm2(out1)\n",
    "        ffn_output = self.ffn(out_norm)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return out1 + ffn_output\n",
    "\n",
    "class PatchEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_patches, embed_dim, **kwargs):\n",
    "        super(PatchEmbedding, self).__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.embed_dim = embed_dim\n",
    "        self.projection = Dense(embed_dim)\n",
    "        self.position_embedding = self.add_weight(\n",
    "            name=\"pos_embed\",\n",
    "            shape=(1, num_patches, embed_dim),\n",
    "            initializer=\"random_normal\"\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        x = self.projection(patch)\n",
    "        return x + self.position_embedding\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PatchEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            'num_patches': self.num_patches,\n",
    "            'embed_dim': self.embed_dim\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b16ebf44-5fbb-4d78-a14b-62a121631890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define custom objects to pass to the load_model function\n",
    "custom_objects = {\n",
    "    'TransformerBlock': TransformerBlock,\n",
    "    'PatchEmbedding': PatchEmbedding\n",
    "}\n",
    "\n",
    "# Load the model\n",
    "vit_model = load_model(MODEL_PATH, custom_objects=custom_objects)\n",
    "print(\"ViT model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375e305e-e86b-445b-a3ff-ed5c46b09df0",
   "metadata": {},
   "source": [
    "## Load Labels and Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b601595-ecd0-4320-90ad-487ef4caaba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 502 labels.\n",
      "Loaded 350 sentences.\n"
     ]
    }
   ],
   "source": [
    "labels_df = pd.read_excel(LABELS_PATH)\n",
    "w2id = dict(zip(labels_df['Sign-Arabic'], labels_df['SignID']))\n",
    "id2w = {v: k for k, v in w2id.items()}\n",
    "words = list(w2id.keys())\n",
    "label_map = {word: idx for idx, word in enumerate(words)}\n",
    "NUM_CLASSES = len(words)\n",
    "print(f\"Loaded {NUM_CLASSES} labels.\")\n",
    "\n",
    "all_sentences_df = pd.read_excel(ALL_SENTENCES_PATH)\n",
    "all_sentences_df.columns = all_sentences_df.columns.str.strip()\n",
    "sentences = all_sentences_df['Sentences'].tolist()\n",
    "print(f\"Loaded {len(sentences)} sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a409928b-f2f4-49a0-b646-e6b63b9b2ee9",
   "metadata": {},
   "source": [
    "## Preprocessing Functions (Arabic Stemmer and Similarity Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d81303e1-4b19-45ae-98bb-c154752b1b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "def normalize(word):\n",
    "    for _ in range(3):\n",
    "        word = re.sub(\"أ\", \"ا\", word)\n",
    "        word = re.sub(\"ي\", \"ى\", word)\n",
    "        word = re.sub(\"ؤ\", \"ء\", word)\n",
    "        word = re.sub(\"ئ\", \"ء\", word)\n",
    "        word = re.sub(\"ة\", \"ه\", word)\n",
    "        word = re.sub(\"گ\", \"ك\", word)\n",
    "    return word\n",
    "\n",
    "def Def_articles_removal(word):\n",
    "    articles = ['بال', 'فال', 'وال', 'كال', 'ولل', 'ال', 'ل', 'لي', 'ا', 'فبال', 'لبال', 'وبال']\n",
    "    for article in articles:\n",
    "        if word.startswith(article):\n",
    "            word = word.replace(article, '')\n",
    "    return word\n",
    "\n",
    "def prefix_removal(word):\n",
    "    p1 = [\"و\", \"ف\", \"ب\", \"ك\", \"ل\"]\n",
    "    p2 = ['أل']\n",
    "    p3 = ['وال', 'فال', 'كال']\n",
    "    for p in p1 + p2 + p3:\n",
    "        if word.startswith(p):\n",
    "            word = word.replace(p, '')\n",
    "    return word\n",
    "\n",
    "def suffix_removal(word):\n",
    "    s1 = ['ي ', 'ك', 'ـه']\n",
    "    s2 = [\"هن\", \"ها\", \"هم\", \"كن\"]\n",
    "    s3 = [\"هما\"]\n",
    "    for s in s1 + s2 + s3:\n",
    "        if word.endswith(s):\n",
    "            word = word.replace(s, '')\n",
    "    return word\n",
    "\n",
    "def Arabic_stemmer(text):\n",
    "    root = []\n",
    "    for s in text:\n",
    "        s1 = normalize(s)\n",
    "        s2 = Def_articles_removal(s1)\n",
    "        s3 = prefix_removal(s2)\n",
    "        s4 = suffix_removal(s3)\n",
    "        root.append(s4)\n",
    "    return root\n",
    "\n",
    "def get_most_similar_sentence(sentences, sign_words):\n",
    "    input_text = ' '.join(Arabic_stemmer(sign_words))\n",
    "    normalized_sentences = [' '.join(Arabic_stemmer(sentence.split())) for sentence in sentences]\n",
    "    combined_sentences = normalized_sentences + [input_text]\n",
    "    tfidf_matrix = vectorizer.fit_transform(combined_sentences)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "    most_similar_idx = cosine_sim.argmax()\n",
    "    return sentences[most_similar_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c73f35d-a99a-4eac-b1e8-5301275caf5d",
   "metadata": {},
   "source": [
    "## Keypoint Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c87b014a-2440-4790-a30f-24a8a7255d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    \"\"\"\n",
    "    This function converts the image to RGB, performs keypoint extraction\n",
    "    using MediaPipe's Holistic model, and converts the image back to BGR.\n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert image to RGB\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)  # Run keypoint extraction\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Convert image back to BGR\n",
    "    return image, results\n",
    "\n",
    "def adjust_landmarks(arr, center):\n",
    "    \"\"\"\n",
    "    This function adjusts landmarks by centering them around a reference point (like the nose or wrist).\n",
    "    \"\"\"\n",
    "    arr_reshaped = arr.reshape(-1, 3)  # Reshape to 2D array\n",
    "    center_repeated = np.tile(center, (len(arr_reshaped), 1))  # Repeat the center point\n",
    "    arr_adjusted = arr_reshaped - center_repeated  # Subtract the center\n",
    "    arr_adjusted = arr_adjusted.reshape(-1)  # Flatten back to 1D array\n",
    "    return arr_adjusted\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    \"\"\"\n",
    "    This function extracts keypoints for the pose, left hand, and right hand from the MediaPipe results.\n",
    "    It flattens the landmarks for each part and adjusts their positions based on a reference point.\n",
    "    \"\"\"\n",
    "    # Extract keypoints for pose, left hand, and right hand\n",
    "    pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "    # Extract wrist positions and nose for normalization purposes\n",
    "    nose = pose[:3]  # The first three values represent the nose position\n",
    "    lh_wrist = lh[:3]  # Left wrist position (first three values)\n",
    "    rh_wrist = rh[:3]  # Right wrist position (first three values)\n",
    "\n",
    "    # Adjust landmarks based on wrist/nose position for better consistency across images\n",
    "    pose_adjusted = adjust_landmarks(pose, nose)\n",
    "    lh_adjusted = adjust_landmarks(lh, lh_wrist)\n",
    "    rh_adjusted = adjust_landmarks(rh, rh_wrist)\n",
    "\n",
    "    return pose_adjusted, lh_adjusted, rh_adjusted\n",
    "\n",
    "def process_single_image(image_path):\n",
    "    \"\"\"\n",
    "    This function processes a single image by extracting keypoints multiple times to create a sequence of keypoints.\n",
    "    This is important to simulate multiple frames and give better data to the model.\n",
    "    \"\"\"\n",
    "    keypoints_seq = []  # Initialize list to store keypoint sequences\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        frame = cv2.imread(image_path)  # Read the image\n",
    "        if frame is None:\n",
    "            raise ValueError(f\"Could not read image {image_path}\")  # Raise an error if the image cannot be read\n",
    "        \n",
    "        # Process the same image multiple times to get F_AVG frames\n",
    "        for _ in range(F_AVG):\n",
    "            image, results = mediapipe_detection(frame.copy(), holistic)  # Detect keypoints in the image\n",
    "            pose, lh, rh = extract_keypoints(results)  # Extract pose and hand keypoints\n",
    "            combined = np.concatenate([pose, lh, rh])  # Combine the keypoints from all parts\n",
    "            keypoints_seq.append(combined)  # Add the keypoints sequence\n",
    "\n",
    "    return np.array(keypoints_seq)  # Return the keypoints sequence as a numpy array\n",
    "\n",
    "def predict_single_image(vit_model, labels_df, keypoints_data):\n",
    "    \"\"\"\n",
    "    This function takes the keypoints data, reshapes it, and predicts the sign using the ViT model.\n",
    "    \"\"\"\n",
    "    input_data = keypoints_data.reshape(1, F_AVG, -1)  # Reshape to match model input shape\n",
    "    predictions = vit_model.predict(input_data)  # Get predictions from the ViT model\n",
    "    predicted_index = np.argmax(predictions, axis=1)[0]  # Get the index of the highest prediction score\n",
    "    return labels_df['Sign-Arabic'][predicted_index]  # Return the predicted sign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b13d019-a973-44a6-94d9-dc28ee3fe79a",
   "metadata": {},
   "source": [
    "## Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17e3d3f2-b009-4c93-a282-6826d4f3b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_image(image_path):\n",
    "    keypoints_seq = []\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        frame = cv2.imread(image_path)\n",
    "        if frame is None:\n",
    "            raise ValueError(f\"Could not read image {image_path}\")\n",
    "        \n",
    "        # Process the same image multiple times to get F_AVG frames\n",
    "        for _ in range(F_AVG):\n",
    "            image, results = mediapipe_detection(frame.copy(), holistic)\n",
    "            pose, lh, rh = extract_keypoints(results)\n",
    "            combined = np.concatenate([pose, lh, rh])\n",
    "            keypoints_seq.append(combined)\n",
    "    \n",
    "    return np.array(keypoints_seq)\n",
    "\n",
    "def predict_single_image(vit_model, labels_df, keypoints_data):\n",
    "    input_data = keypoints_data.reshape(1, F_AVG, -1)\n",
    "    predictions = vit_model.predict(input_data)\n",
    "    predicted_index = np.argmax(predictions, axis=1)[0]\n",
    "    return labels_df['Sign-Arabic'][predicted_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ac5f64-7230-43ec-b5b2-76e8fe965107",
   "metadata": {},
   "source": [
    "## GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b09a93d-f86c-4537-b7c0-717f74dfc3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n"
     ]
    }
   ],
   "source": [
    "class TranslationResultPopup(tk.Toplevel):\n",
    "    def __init__(self, parent, uploaded_images, predicted_signs, translation):\n",
    "        super().__init__(parent)\n",
    "        self.title(\"Translation Result\")  # Window title\n",
    "        self.geometry(\"900x700\")  # Window size\n",
    "        self.resizable(False, False)  # Disable resizing\n",
    "        self.configure(bg='#f0f0f0')  # Background color\n",
    "\n",
    "        # Main frame with scrollbar for displaying the images and results\n",
    "        main_frame = tk.Frame(self, bg='#f0f0f0')\n",
    "        main_frame.pack(fill=tk.BOTH, expand=True, padx=20, pady=20)\n",
    "        \n",
    "        # Create a canvas to hold the scrollable content\n",
    "        canvas = tk.Canvas(main_frame, bg='#f0f0f0')\n",
    "        scrollbar = ttk.Scrollbar(main_frame, orient=\"vertical\", command=canvas.yview)\n",
    "        scrollable_frame = tk.Frame(canvas, bg='#f0f0f0')\n",
    "        \n",
    "        # Update the scrollable area as content changes\n",
    "        scrollable_frame.bind(\n",
    "            \"<Configure>\",\n",
    "            lambda e: canvas.configure(\n",
    "                scrollregion=canvas.bbox(\"all\")\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        canvas.create_window((0, 0), window=scrollable_frame, anchor=\"nw\")\n",
    "        canvas.configure(yscrollcommand=scrollbar.set)\n",
    "        \n",
    "        canvas.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "        scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "        \n",
    "        # Title label\n",
    "        title_label = tk.Label(scrollable_frame, text=\"Translation Result\", \n",
    "                             font=(\"Arial\", 18, \"bold\"), bg='#f0f0f0')\n",
    "        title_label.pack(pady=10)\n",
    "        \n",
    "        # Frame to hold uploaded images and predictions\n",
    "        images_frame = tk.Frame(scrollable_frame, bg='#f0f0f0')\n",
    "        images_frame.pack(fill=tk.X, pady=10)\n",
    "        \n",
    "        # Display each uploaded image with its predicted sign in a grid\n",
    "        for i, (img_path, sign) in enumerate(zip(uploaded_images, predicted_signs)):\n",
    "            try:\n",
    "                # Create a frame for each image-prediction pair\n",
    "                pair_frame = tk.Frame(images_frame, bg='#f0f0f0', padx=10, pady=10)\n",
    "                pair_frame.grid(row=i//3, column=i%3, padx=10, pady=10)\n",
    "                \n",
    "                # Load, resize, and display the image\n",
    "                img = Image.open(img_path)\n",
    "                img.thumbnail((200, 200), Image.LANCZOS)\n",
    "                photo = ImageTk.PhotoImage(img)\n",
    "                \n",
    "                img_label = tk.Label(pair_frame, image=photo, bg='white')\n",
    "                img_label.image = photo\n",
    "                img_label.pack()\n",
    "                \n",
    "                # Label showing the image number\n",
    "                img_num = tk.Label(pair_frame, text=f\"Image {i+1}\", \n",
    "                                  font=(\"Arial\", 10), bg='#f0f0f0')\n",
    "                img_num.pack()\n",
    "                \n",
    "                # Label showing the predicted sign\n",
    "                pred_label = tk.Label(pair_frame, text=f\"Predicted: {sign}\", \n",
    "                                    font=(\"Arial\", 12), bg='#f0f0f0')\n",
    "                pred_label.pack(pady=5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Handle errors if any image fails to load\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "                error_label = tk.Label(pair_frame, text=f\"Image {i+1} not found\", \n",
    "                                     font=(\"Arial\", 12), bg='#f0f0f0')\n",
    "                error_label.pack()\n",
    "        \n",
    "        # Frame for displaying the full translation\n",
    "        translation_frame = tk.Frame(scrollable_frame, bg='#f0f0f0')\n",
    "        translation_frame.pack(fill=tk.X, pady=20)\n",
    "        \n",
    "        tk.Label(translation_frame, text=\"Full Translation:\", \n",
    "                font=(\"Arial\", 14, \"bold\"), bg='#f0f0f0').pack(anchor='w')\n",
    "        \n",
    "        # Display the full translation in a non-editable text box\n",
    "        translation_text = tk.Text(translation_frame, height=5, width=80, \n",
    "                                 font=(\"Arial\", 12), wrap=tk.WORD, padx=10, pady=10)\n",
    "        translation_text.insert(tk.END, translation)\n",
    "        translation_text.config(state=tk.DISABLED)\n",
    "        translation_text.pack(fill=tk.X, pady=10)\n",
    "        \n",
    "        # Close button for the popup\n",
    "        close_btn = tk.Button(scrollable_frame, text=\"Close\", font=(\"Arial\", 12),\n",
    "                            command=self.destroy)\n",
    "        close_btn.pack(pady=10)\n",
    "\n",
    "class SignLanguageApp(tk.Tk):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.title(\"Sign Language Prediction\")  # Window title\n",
    "        self.geometry(\"900x600\")  # Window size\n",
    "        self.resizable(False, False)  # Disable resizing\n",
    "        self.configure(bg='#f0f0f0')  # Background color\n",
    "        self.uploaded_images = []  # List to store uploaded images\n",
    "        self.num_images = 0  # Number of images to upload\n",
    "        self.current_image = 0  # Track the current image number\n",
    "\n",
    "        # Load background image if it exists\n",
    "        if os.path.exists(BACKGROUND_PATH):\n",
    "            bg_image = Image.open(BACKGROUND_PATH)\n",
    "            bg_image = bg_image.resize((900, 600), Image.LANCZOS)\n",
    "            self.bg_photo = ImageTk.PhotoImage(bg_image)\n",
    "            bg_label = tk.Label(self, image=self.bg_photo)\n",
    "            bg_label.place(x=0, y=0, relwidth=1, relheight=1)\n",
    "        else:\n",
    "            print(\"Background image not found at\", BACKGROUND_PATH)\n",
    "\n",
    "        # Button styling\n",
    "        button_bg = '#f0f0f0'\n",
    "        button_fg = 'black'\n",
    "        button_font = (\"Arial\", 14)\n",
    "        \n",
    "        # Button to start live camera caption (coming soon)\n",
    "        live_btn = tk.Button(\n",
    "            self,\n",
    "            text=\"Live Camera Caption\",\n",
    "            font=button_font,\n",
    "            bg=button_bg,\n",
    "            fg=button_fg,\n",
    "            activebackground='gray',\n",
    "            activeforeground='black',\n",
    "            command=self.live_camera\n",
    "        )\n",
    "        live_btn.place(x=450, y=420, anchor=\"n\")\n",
    "\n",
    "        # Button to upload images\n",
    "        upload_btn = tk.Button(\n",
    "            self,\n",
    "            text=\"Upload Images\",\n",
    "            font=button_font,\n",
    "            bg=button_bg,\n",
    "            fg=button_fg,\n",
    "            activebackground='gray',\n",
    "            activeforeground='black',\n",
    "            command=self.upload_images\n",
    "        )\n",
    "        upload_btn.place(x=450, y=470, anchor=\"n\")\n",
    "\n",
    "    def live_camera(self):\n",
    "        # Show message when live camera feature is clicked (coming soon)\n",
    "        messagebox.showinfo(\"Info\", \"This feature coming soon! Stay tuned😊\")\n",
    "\n",
    "    def upload_images(self):\n",
    "        # Create a new window for uploading images\n",
    "        upload_window = tk.Toplevel(self)\n",
    "        upload_window.title(\"Upload Images\")\n",
    "        upload_window.geometry(\"600x400\")\n",
    "        upload_window.grab_set()  # Make this window modal (blocks interaction with the main window)\n",
    "        \n",
    "        # Guide message for number of images to upload\n",
    "        self.status_var = tk.StringVar()\n",
    "        self.status_var.set(f\"Select number of images to upload ({MIN_IMAGES}-{MAX_IMAGES})\")\n",
    "        status_label = tk.Label(upload_window, textvariable=self.status_var, \n",
    "                               font=(\"Arial\", 12), wraplength=500)\n",
    "        status_label.pack(pady=20)\n",
    "\n",
    "        # Variable for number of images to upload\n",
    "        image_var = tk.IntVar(upload_window)\n",
    "        image_var.set(MIN_IMAGES)\n",
    "\n",
    "        # Label and combo box to select number of images\n",
    "        tk.Label(upload_window, text=f\"Select number of images ({MIN_IMAGES}-{MAX_IMAGES}):\", \n",
    "                font=(\"Arial\", 12)).pack(pady=10)\n",
    "        image_combo = ttk.Combobox(upload_window, textvariable=image_var, \n",
    "                                 values=list(range(MIN_IMAGES, MAX_IMAGES+1)), state=\"readonly\")\n",
    "        image_combo.pack(pady=5)\n",
    "\n",
    "        # Button to start uploading the images\n",
    "        start_btn = tk.Button(upload_window, text=\"Start Upload\", font=(\"Arial\", 12),\n",
    "                            command=lambda: self.start_image_selection(upload_window, image_var.get()))\n",
    "        start_btn.pack(pady=20)\n",
    "        \n",
    "        # Cancel button to close the upload window\n",
    "        cancel_btn = tk.Button(upload_window, text=\"Cancel\", font=(\"Arial\", 12),\n",
    "                             command=upload_window.destroy)\n",
    "        cancel_btn.pack(pady=10)\n",
    "\n",
    "    def start_image_selection(self, parent_window, num_images):\n",
    "        # Start the process of selecting images\n",
    "        self.num_images = num_images\n",
    "        self.current_image = 1\n",
    "        self.uploaded_images = []  # Clear any previously uploaded images\n",
    "        self.status_var.set(f\"Select image {self.current_image} of {self.num_images}\")\n",
    "        self.select_next_image(parent_window)\n",
    "\n",
    "    def select_next_image(self, parent_window):\n",
    "        # Open file dialog to select an image\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            title=f\"Select image {self.current_image}/{self.num_images}\",\n",
    "            filetypes=[(\"Image Files\", \"*.png *.jpg *.jpeg\")]\n",
    "        )\n",
    "        \n",
    "        if file_path:\n",
    "            # Append the selected image to the list\n",
    "            self.uploaded_images.append(file_path)\n",
    "            filename = os.path.basename(file_path)\n",
    "            self.status_var.set(\n",
    "                f\"Selected image {self.current_image}: {filename}\\n\"\n",
    "                f\"Select image {self.current_image+1} of {self.num_images}\"\n",
    "            )\n",
    "            \n",
    "            self.current_image += 1\n",
    "            \n",
    "            if self.current_image <= self.num_images:\n",
    "                # Continue selecting the next image\n",
    "                parent_window.after(100, lambda: self.select_next_image(parent_window))\n",
    "            else:\n",
    "                # Process images when all have been selected\n",
    "                ok_btn = tk.Button(parent_window, text=\"Process Images\", font=(\"Arial\", 12),\n",
    "                                  command=lambda: self.process_and_show_result(parent_window))\n",
    "                ok_btn.pack(pady=20)\n",
    "        else:\n",
    "            if self.current_image == 1:\n",
    "                # Close the window if the first image is canceled\n",
    "                parent_window.destroy()\n",
    "            else:\n",
    "                self.status_var.set(\n",
    "                    f\"Selection canceled for image {self.current_image}\\n\"\n",
    "                    f\"Please select image {self.current_image} of {self.num_images}\"\n",
    "                )\n",
    "\n",
    "    def process_and_show_result(self, parent_window):\n",
    "        # Check if enough images are uploaded\n",
    "        if not self.uploaded_images or len(self.uploaded_images) < MIN_IMAGES or len(self.uploaded_images) > MAX_IMAGES:\n",
    "            messagebox.showerror(\"Error\", \n",
    "                                f\"Please upload between {MIN_IMAGES} and {MAX_IMAGES} images.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            predicted_signs = []\n",
    "            \n",
    "            # Process each uploaded image\n",
    "            for img_path in self.uploaded_images:\n",
    "                # Get keypoints for this image\n",
    "                keypoints_seq = process_single_image(img_path)\n",
    "                \n",
    "                # Predict sign for this image\n",
    "                predicted_sign = predict_single_image(vit_model, labels_df, keypoints_seq)\n",
    "                predicted_signs.append(predicted_sign)\n",
    "            \n",
    "            # Get the most similar sentence for the predicted words\n",
    "            most_similar = get_most_similar_sentence(sentences, predicted_signs)\n",
    "            \n",
    "            # Close the upload window\n",
    "            parent_window.destroy()\n",
    "            \n",
    "            # Show the result in a new popup window\n",
    "            TranslationResultPopup(self, self.uploaded_images, predicted_signs, most_similar)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Show error message if something goes wrong\n",
    "            messagebox.showerror(\"Error\", f\"Failed to process images: {str(e)}\")\n",
    "\n",
    "# Main code to run the application\n",
    "if __name__ == \"__main__\":\n",
    "    app = SignLanguageApp()\n",
    "    app.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
